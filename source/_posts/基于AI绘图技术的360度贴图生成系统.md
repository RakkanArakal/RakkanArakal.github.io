---
title: 基于AI绘图技术的360度贴图生成
index_img: /img/Panorama.png
banner_img: /img/bg.jpg
date: 2023-01-23 16:56:53
tag: 技术
---


# 基于AI绘图技术的360度贴图生成



## 项目流程图
![](https://notes.sjtu.edu.cn/uploads/upload_88145f7d70d9f616f8c373f2fa4f3f1c.png)


## Unity客户端部分

采用Unity提供的自定义Editor的方式，编写Unity编辑器插件，实现上传文字prompt和选择图片上传到服务器后端的功能。

对于服务器返回的图片，实现了自定义Asset的导入流程，使图片自动转化为可以被直接使用的天空盒Material。在运行时实现了读取目录下Material并供用户选择的功能。

## 后端服务部分

基于Python的Flask框架实现。接收客户端发来的文字或者图片，经过BLIP、CLIP（文字提取）、Text2Light（全景图生成）、stable-diffusion（局部细节修复）等模型，生成一张与原图相关联的全景图，并返回给客户端。



## Text2Light

全景图不同于一般风景图像视野有限，可以看作是360度球面场。这一约束是全景图生成的基础，模型中引入了球形位置编码来表示360度场景。将全景图上的像素映射到球面上的一点上，保证了结构的完整
![](https://notes.sjtu.edu.cn/uploads/upload_0335bc9001960b8d564f2402a1359a75.png)


模型主要有三个部分，首先构建双码本，对于512\*1024的全景图，下采样成128\*256来构建表示整体外观和语义的全局码本$Z_g$，从全景图中随机裁取256\*256的补丁来构建表示局部细粒度特征的局部码本$Z_l$。
![](https://notes.sjtu.edu.cn/uploads/upload_6014afd01d32d121d5777cd6297ed0e9.png)



第二部分是以输入文本为条件，在全局码本中采样整体语义。使用CLIP的文本编码器和图像编码器，训练时通过扰动图像特征生成伪文本特征，文本条件采样器从给定输入文本的全局码本$Z_g$中采样整体特征，以自回归的方式训练全局采样器，采样器会预测可能的下一个指标s的分布，使得文本对齐采样器的可能性最大化，在推理过程中，直接使用真实文本特征，得到整体条件$z_g$。
![](https://notes.sjtu.edu.cn/uploads/upload_d26a0760ef81b1a0588d8e8e00a2d843.png)



最后在整体条件的指导下，逐片合成结构感知的全景图。具体的说，就是从局部码本$Z_l$中采样特征。在给定整体条件$z_g$的情况下，合成对应球形位置编码的局部碎片。
![](https://notes.sjtu.edu.cn/uploads/upload_cb85929dfea706edf0e52d55c2de6c61.png)

## BLIP
对于从图片到全景图，我们考虑的第一个技术路线是先从图片提取描述，这样就可以使用Text2Light模型，生成全景图。

图像描述相关的研究已经较为成熟，并且在帮助视障人士描绘环境物体等方面得到了应用。
![](https://notes.sjtu.edu.cn/uploads/upload_546988233860a9d70e228c66437a2884.png)
我们使用的BLIP模型是一个全新的VLP框架，它可以统一视觉语言理解和生成，在各种下游任务上取得了稳定的性能改进，它使用多模态混合编码器-解码器，其中的语言建模损失以自回归的方式最大化文本的概率，激活了以图像为基础的文本解码器，能够生成给定图像的文本描述。

## CLIP
无论是Text2Light或是BLIP，都离不开CLIP的帮助，CLIP是一种基于对比文本-图像对的预训练方法或者模型，是一种基于对比学习的多模态模型。
![](https://notes.sjtu.edu.cn/uploads/upload_fbdfa6c24adcb55042fb6cc25b604fae.png)


## Stable-Diffusion
### 扩散模型
![](https://notes.sjtu.edu.cn/uploads/upload_42983736bebbf3063e8a5aeded35d7df.png)
扩散模型受热力学启发，通过反转逐渐的噪声过程来学习生成数据。如上图所示，分为扩散过程（forward/diffusion process）和逆扩散过程（reverse process）。

* 扩散过程（$X_0 - X_T$）：逐步对图像加噪声，这一逐步过程可以认为是参数化的马尔可夫过程。
* 逆扩散过程（$X_T - X_0$）：从噪声中反向推导，逐渐消除噪声以逆转生成图像。

训练完成后，就能通过随机采样高斯噪声来生成图像了。实际上扩散模型和AE、VAE很类似，一个粗略的发展过程可以认为是AE–VAE–VQVAE–Diffusion，而扩散模型也逐步从DDPM–GLIDE–DALLE2–Stable Diffusion。

![](https://notes.sjtu.edu.cn/uploads/upload_f4348ecf5ab96fdaf605e39aaa072921.png)

diffusion训练的核心就是取学习高斯噪声$z_t$与$z_θ$之间的MSE均方误差。

### **GLIDE**（ Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models）

![](https://notes.sjtu.edu.cn/uploads/upload_7e3e36c3ff6305c43b642049839cdd5a.png)

OpenAI在2021提出了GLIDE模型，使用了无分类器引导的数据做扩散引导，在每一步的扩散过程中加入了Transformer的最后一个token作为条件。

GLIDE模型最重要的贡献在于它允许文本作为条件来生成图像。

### **DALLE·2**（Hierarchical Text-conditional Image Generation with Clip latents）
![](https://notes.sjtu.edu.cn/uploads/upload_8f57de79d991ad28b7a225b4167f2ed2.png)
DALLE2的模型结构如图，其中生成图像的扩散过程是基于GLIDE实现的。

* 虚线上半部分是预训练好的CLIP。一侧输入文本，一侧是图像，用于得到表征。
* 虚线下半部分是text-to-image的生成过程。这一过程是二阶的过程，即文本变图像特征，再特性特征变图像。首先文本特征输入autoregressive或者diffusion prior以得到初步的图像特征（实验证明diffusion效率更高，因此一般选用diffusion），然后该特征会进一步作为condition到反向扩散模型中生成最后的图片。

值得注意的是 GLIDE 模型以两种方式使用投影的 CLIP 文本嵌入。第一种是将它们添加到 GLIDE 现有的时间步嵌入中，第二种是通过创建四个额外的上下文 token，它们连接到 GLIDE 文本编码器的输出序列。

### **Stable-Diffusion**（High-Resolution Image Synthesis with Latent Diffusion Models）

![](https://notes.sjtu.edu.cn/uploads/upload_61ee3dbce74c0a24873fc439db9c9351.png)


Stable-Diffusion是一个基于Latent Diffusion Models（潜在扩散模型，LDMs）的文图生成（text-to-image）模型。
构成Stable Diffusion的三个主要组成部分：
* **ClipText**：用于对输入进行文本编码
* **UNet + Scheduler**：在信息(潜在)空间中逐步处理/传播信息。
* **Autoencoder Decoder**：使用处理过的信息数组绘制最终图像。

为了加快图像生成过程，Stable-Diffusion运行的扩散过程不是在像素图像本身，而是在图像的压缩版本。这种压缩(以及后来的解压缩/绘制)是通过自动编码器完成的。自动编码器使用编码器将图像压缩到潜在空间，然后使用解码器仅使用压缩信息重建图像。


## 风格迁移

针对传入的原图和全景图风格差异过大的问题，考虑采用风格迁移的方式保留全景图的内容。

Gatys等人在2015年首先使用深度学习进行艺术画风格学习。输入一张内容图和一张风格图，输出一张保留内容图的内容和风格图的风格的图片。

在卷积神经网络CNN中，高层特征代表着图像的内容，由此为了保留图像的内容，定义内容损失函数为欧氏距离：
$$
L_{content}(\vec p,\vec x,l)=\frac{1}{2}\sum_{i,j}(F^i_{ij}-P^i_{ij})^2
$$
风格特征在CNN中为低层特征，利用Gram矩阵定义风格损失函数如下：
$$
G^l_{ij}=\sum_k F^l_{ik}F^l_{jk}\\
E_l=\frac{1}{4N^2_lM^2_l}\sum_{ij}(G^l_{ij}-A^l_{ij})^2
\\L_{style}(\vec a, \vec x) = \sum^L_{l=0}w_lE_l
$$
总损失为内容损失与风格损失的加权和。

Luan在CVPR2017上发表的Deep Photo Style Transfer论文中提出一种对照片（摄影作品）的风格迁移方法，在损失函数中引入由拉普拉斯矩阵得出的正则项。先使用ResNet对原图和风格图进行语义分割，生成一些遮罩（mask）并增强神经网络的风格算法，输入到VGG19卷积神经网络中。

我们使用该方法尝试对全景图效果进行优化，效果如下，其中中间为风格图（客户端输入图片），左侧为内容图（根据输入图片生成的六面图），右侧为风格迁移的输出：
![](https://notes.sjtu.edu.cn/uploads/upload_687ef12e0a48d538d911068a0e6d43ac.png)


整体效果尚可，但是存在语义分割不精确导致细节失真，且运行时间长的问题，因此风格迁移部分作为系统可选的模块。

此外，最近流行的生成式模型（CycleGAN等）可以在没有风格图输入的情况下进行风格迁移，且运行速度较快，但是由于需要时间和精力收集数据集进行训练，我们没有采用该方法。

## 最终方案
![](https://notes.sjtu.edu.cn/uploads/upload_98980a1a00677c9a550c66e96830c4ea.png)
最终我们采用的方案是：
1. 输入一张图片或者文字，如果是图片的话通过CLIP+BLIP获取该图的语义信息
2. 通过Text2Light，输入该语义生成全景图
3. 讲全景图根据极坐标转化为六方格图，提取出其接缝有问题的部分
4. 将接缝问题图和mask一同传入diffusion中进行重绘，获得最后的成品图

最后的图片返回至unity中可以通过我们写的脚本直接转化为mat格式供客户端展示。详细运行流程可以观看我们的演示视频。


## 参考文献
1. Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger and Ilya Sutskever. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning (2021).
1. Chen, Zhaoxi, Guangcong Wang and Ziwei Liu. “Text2Light: Zero-Shot Text-Driven HDR Panorama Generation.” ACM Trans. Graph. 41 (2022): 195:1-195:16.
1. Gatys, Leon A., Alexander S. Ecker and Matthias Bethge. “Image Style Transfer Using Convolutional Neural Networks.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016): 2414-2423.
1. Luan, Fujun, Sylvain Paris, Eli Shechtman and Kavita Bala. “Deep Photo Style Transfer.” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017): 6997-7005.
2. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021). Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020
3. Nichol, Alex, et al. "Glide: Towards photorealistic image generation and editing with text-guided diffusion models." arXiv preprint arXiv:2112.10741 (2021).
4. Ho, Jonathan, Ajay Jain, and Pieter Abbeel. "Denoising diffusion probabilistic models." Advances in Neural Information Processing Systems 33 (2020): 6840-6851.
5. Ramesh, Aditya, et al. "Hierarchical text-conditional image generation with clip latents." arXiv preprint arXiv:2204.06125 (2022).
6. Rombach, Robin, et al. "High-resolution image synthesis with latent diffusion models." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.
